{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac97bb46",
   "metadata": {},
   "source": [
    "# Llama-2-7B + PEFT LoRA 微调 PokerBench 分类任务\n",
    "\n",
    "此 Notebook 演示如何在 Kaggle 环境下，使用 Llama-2-7B 模型，结合 PEFT 技术进行扑克动作分类的微调训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01eab5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. 环境依赖安装\n",
    "!pip install transformers datasets accelerate peft bitsandbytes scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43371c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import LlamaTokenizer, LlamaForSequenceClassification, get_scheduler, AdamW\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed90d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 加载 PokerBench 数据集\n",
    "with open('/kaggle/input/pokerbench/pokerbench_data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "texts = [item['instruction'] for item in data]\n",
    "labels_raw = [item['output'] for item in data]\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels_raw)\n",
    "num_labels = len(label_encoder.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182ea0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 初始化 Tokenizer 和模型\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "model = LlamaForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    "    num_labels=num_labels,\n",
    ")\n",
    "\n",
    "model = prepare_model_for_int8_training(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591239b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 配置 LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1117fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 构建 Dataset\n",
    "class PokerBenchDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        enc = tokenizer(self.texts[idx], truncation=True, max_length=256, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        enc = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "        enc[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return enc\n",
    "\n",
    "dataset = PokerBenchDataset(texts, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32de6833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 划分训练/验证集\n",
    "train_size = int(len(dataset) * 0.8)\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc70d611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. 优化器和学习率调度\n",
    "optimizer = AdamW(model.parameters(), lr=2e-4)\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_loader)\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277c7eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. 训练和评估函数\n",
    "def train_epoch():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    trues = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            logits = outputs.logits\n",
    "            pred_labels = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "            preds.extend(pred_labels)\n",
    "            trues.extend(batch[\"labels\"].cpu().numpy())\n",
    "    return preds, trues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916f5188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. 主训练循环\n",
    "for epoch in range(num_epochs):\n",
    "    loss = train_epoch()\n",
    "    preds, trues = evaluate()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {loss:.4f}\")\n",
    "    print(classification_report(trues, preds, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc19a01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. 保存模型和 Tokenizer\n",
    "model.save_pretrained('./llama2-7b-pokerbench-peft')\n",
    "tokenizer.save_pretrained('./llama2-7b-pokerbench-peft')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c2d918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. 简单推理示例\n",
    "def predict(text):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(text, truncation=True, max_length=256, padding=\"max_length\", return_tensors=\"pt\").to(device)\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    pred_idx = torch.argmax(logits, dim=-1).item()\n",
    "    return label_encoder.inverse_transform([pred_idx])[0]\n",
    "\n",
    "print(predict(\"You have Ah Kh, the board is 9h Th Jc. What is the best move?\"))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
